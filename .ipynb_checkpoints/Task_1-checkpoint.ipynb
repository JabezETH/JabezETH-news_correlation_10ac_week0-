{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c94d592-846c-45c8-9d4c-5dcd59a82a03",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee2dbb3c-17d7-428c-b489-7de8c4985c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement collections (from versions: none)\n",
      "ERROR: No matching distribution found for collections\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install nltk collections keybert yake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97012d85-2779-4fe4-a8df-368595a591d6",
   "metadata": {},
   "source": [
    "# Reading the csv file and assining a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b08c9c-a1ed-4525-8af9-23b19bf038cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\user\\OneDrive\\Documents\\10 Acadamy\\rating.csv', encoding='utf-8')\n",
    "data_traffic = pd.read_csv(r'C:\\Users\\user\\OneDrive\\Documents\\10 Acadamy\\traffic.csv', encoding='utf-8')\n",
    "data_domain = pd.read_csv(r'C:\\Users\\user\\OneDrive\\Documents\\10 Acadamy\\domains_location.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f389fdc-64d1-4c5a-a859-49088eb36e11",
   "metadata": {},
   "source": [
    "# Checking the csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9815eaa6-c509-4406-bdeb-4a39921f4479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58356 entries, 0 to 58355\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   article_id       58356 non-null  int64 \n",
      " 1   source_id        17771 non-null  object\n",
      " 2   source_name      58356 non-null  object\n",
      " 3   author           56193 non-null  object\n",
      " 4   title            58356 non-null  object\n",
      " 5   description      58346 non-null  object\n",
      " 6   url              58356 non-null  object\n",
      " 7   url_to_image     54905 non-null  object\n",
      " 8   published_at     58356 non-null  object\n",
      " 9   content          58356 non-null  object\n",
      " 10  category         58335 non-null  object\n",
      " 11  article          58356 non-null  object\n",
      " 12  title_sentiment  58356 non-null  object\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaed27c-46b1-4258-8f71-caedfbf97119",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b3f94c-cf65-4f96-b5ae-96c80282aaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_to_image</th>\n",
       "      <th>published_at</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>article</th>\n",
       "      <th>title_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Elizabeth Brownfield, Contributor, \\n Elizabet...</td>\n",
       "      <td>superstar chef yannick alléno brings refined f...</td>\n",
       "      <td>Now open in Mayfair at Four Seasons Hotel Lond...</td>\n",
       "      <td>https://www.forbes.com/sites/elizabethbrownfie...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2023-11-01 03:27:21.000000</td>\n",
       "      <td>Pavyllon London, at Four Seasons Hotel London ...</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>pavyllon london, at four seasons hotel london ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nice claim top spot in ligue 1 with late win a...</td>\n",
       "      <td>Nice moved into provisional first place in the...</td>\n",
       "      <td>https://www.channelnewsasia.com/sport/nice-cla...</td>\n",
       "      <td>https://onecms-res.cloudinary.com/image/upload...</td>\n",
       "      <td>2023-10-27 21:28:48.000000</td>\n",
       "      <td>Nice moved into provisional first place in the...</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>nice moved into provisional first place in the...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81694</td>\n",
       "      <td>time</td>\n",
       "      <td>Time</td>\n",
       "      <td>Christina Larson / AP</td>\n",
       "      <td>amphibians are the world’s most vulnerable spe...</td>\n",
       "      <td>The world’s frogs, salamanders, newts, and oth...</td>\n",
       "      <td>https://time.com/6320467/amphibians-most-vulne...</td>\n",
       "      <td>https://api.time.com/wp-content/uploads/2023/1...</td>\n",
       "      <td>2023-10-04 17:36:18.000000</td>\n",
       "      <td>The worlds frogs, salamanders, newts and other...</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>the world’s frogs, salamanders, newts and othe...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phys.Org</td>\n",
       "      <td>Sara Schmidt</td>\n",
       "      <td>image: rusty red waters in madagascar</td>\n",
       "      <td>Iron-rich sediment colors the red-orange water...</td>\n",
       "      <td>https://phys.org/news/2023-10-image-rusty-red-...</td>\n",
       "      <td>https://scx2.b-cdn.net/gfx/news/2023/image-rus...</td>\n",
       "      <td>2023-10-31 18:04:02.000000</td>\n",
       "      <td>Iron-rich sediment colors the red-orange water...</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>iron-rich sediment colors the red-orange water...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Digital Trends</td>\n",
       "      <td>Jason Struss</td>\n",
       "      <td>everything leaving max (formerly hbo max) in n...</td>\n",
       "      <td>From Gangs of London to Fear the Walking Dead ...</td>\n",
       "      <td>https://www.digitaltrends.com/movies/everythin...</td>\n",
       "      <td>https://www.digitaltrends.com/wp-content/uploa...</td>\n",
       "      <td>2023-10-23 23:09:18.000000</td>\n",
       "      <td>Everything ends. No, I’m not having an existen...</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>everything ends. no, i’m not having an existen...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id source_id     source_name  \\\n",
       "0       81664       NaN          Forbes   \n",
       "1       81667       NaN             CNA   \n",
       "2       81694      time            Time   \n",
       "3       81695       NaN        Phys.Org   \n",
       "4       81703       NaN  Digital Trends   \n",
       "\n",
       "                                              author  \\\n",
       "0  Elizabeth Brownfield, Contributor, \\n Elizabet...   \n",
       "1                                                NaN   \n",
       "2                              Christina Larson / AP   \n",
       "3                                       Sara Schmidt   \n",
       "4                                       Jason Struss   \n",
       "\n",
       "                                               title  \\\n",
       "0  superstar chef yannick alléno brings refined f...   \n",
       "1  nice claim top spot in ligue 1 with late win a...   \n",
       "2  amphibians are the world’s most vulnerable spe...   \n",
       "3              image: rusty red waters in madagascar   \n",
       "4  everything leaving max (formerly hbo max) in n...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Now open in Mayfair at Four Seasons Hotel Lond...   \n",
       "1  Nice moved into provisional first place in the...   \n",
       "2  The world’s frogs, salamanders, newts, and oth...   \n",
       "3  Iron-rich sediment colors the red-orange water...   \n",
       "4  From Gangs of London to Fear the Walking Dead ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.forbes.com/sites/elizabethbrownfie...   \n",
       "1  https://www.channelnewsasia.com/sport/nice-cla...   \n",
       "2  https://time.com/6320467/amphibians-most-vulne...   \n",
       "3  https://phys.org/news/2023-10-image-rusty-red-...   \n",
       "4  https://www.digitaltrends.com/movies/everythin...   \n",
       "\n",
       "                                        url_to_image  \\\n",
       "0  https://imageio.forbes.com/specials-images/ima...   \n",
       "1  https://onecms-res.cloudinary.com/image/upload...   \n",
       "2  https://api.time.com/wp-content/uploads/2023/1...   \n",
       "3  https://scx2.b-cdn.net/gfx/news/2023/image-rus...   \n",
       "4  https://www.digitaltrends.com/wp-content/uploa...   \n",
       "\n",
       "                 published_at  \\\n",
       "0  2023-11-01 03:27:21.000000   \n",
       "1  2023-10-27 21:28:48.000000   \n",
       "2  2023-10-04 17:36:18.000000   \n",
       "3  2023-10-31 18:04:02.000000   \n",
       "4  2023-10-23 23:09:18.000000   \n",
       "\n",
       "                                             content    category  \\\n",
       "0  Pavyllon London, at Four Seasons Hotel London ...      Monaco   \n",
       "1  Nice moved into provisional first place in the...      Monaco   \n",
       "2  The worlds frogs, salamanders, newts and other...  Madagascar   \n",
       "3  Iron-rich sediment colors the red-orange water...  Madagascar   \n",
       "4  Everything ends. No, I’m not having an existen...  Madagascar   \n",
       "\n",
       "                                             article title_sentiment  \n",
       "0  pavyllon london, at four seasons hotel london ...         Neutral  \n",
       "1  nice moved into provisional first place in the...        Positive  \n",
       "2  the world’s frogs, salamanders, newts and othe...        Negative  \n",
       "3  iron-rich sediment colors the red-orange water...         Neutral  \n",
       "4  everything ends. no, i’m not having an existen...         Neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() # to view the top datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a89b1-7649-46ed-a87e-709f0fbcc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail() # to view the bottom datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f5cc0-4019-4d9c-bba0-7061758b44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0402c-00a1-4964-a865-618f9f9a2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traffic.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad61312-5797-4dea-b9ce-a64f3ded8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7ab44-462f-43df-95ec-5c6c6ec15347",
   "metadata": {},
   "source": [
    "# Task 1- Websites that have the largest count of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d8c4ed-3ff4-42b4-af06-a0a2bc174f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ten website by number of article published:\n",
      "website\n",
      "www.etfdailynews.com            16746\n",
      "www.globenewswire.com            5423\n",
      "economictimes.indiatimes.com     5310\n",
      "www.globalsecurity.org           3119\n",
      "www.forbes.com                   2784\n",
      "timesofindia.indiatimes.com      2194\n",
      "abcnews.go.com                   2058\n",
      "www.businessinsider.com          2034\n",
      "www.bbc.co.uk                    2032\n",
      "punchng.com                      1800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom Ten websites by number of article published:\n",
      "website\n",
      "gizmodo.com                         388\n",
      "readwrite.com                       324\n",
      "www.euronews.com                    286\n",
      "www.wired.com                       270\n",
      "www.cnn.com                         267\n",
      "www.theverge.com                    214\n",
      "www.bbc.com                          81\n",
      "allafrica.com                        20\n",
      "cnalifestyle.channelnewsasia.com     18\n",
      "cnaluxury.channelnewsasia.com         4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "def extract_website(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "data['website'] = data['url'].apply(extract_website)\n",
    "website_counts = data['website'].value_counts()\n",
    "sorted_website_data = website_counts.sort_values(ascending=False)\n",
    "top_websites = sorted_website_data.head(10)\n",
    "bottom_websites = sorted_website_data.tail(10)\n",
    "print(\"Top Ten website by number of article published:\")\n",
    "print(top_websites)\n",
    "print(\"\\nBottom Ten websites by number of article published:\")\n",
    "print(bottom_websites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049480f-7afa-4be2-9537-c66324037bf3",
   "metadata": {},
   "source": [
    "# Task 1: Websites with the highest numbers of visitors traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c736e6-ca69-4dfa-83e7-17992336fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ten Domain Names by GlobalRank:\n",
      "   GlobalRank                Domain\n",
      "0           1            google.com\n",
      "1           2          facebook.com\n",
      "2           3           youtube.com\n",
      "3           4           twitter.com\n",
      "4           5         instagram.com\n",
      "5           6          linkedin.com\n",
      "6           7             apple.com\n",
      "7           8         microsoft.com\n",
      "8           9  googletagmanager.com\n",
      "9          10         wikipedia.org\n",
      "\n",
      "Bottom Ten Domain Names by GlobalRank:\n",
      "        GlobalRank                  Domain\n",
      "999990      999991            eiretrip.com\n",
      "999991      999992    exploring-africa.com\n",
      "999992      999993                hmag.com\n",
      "999993      999994          irishcycle.com\n",
      "999994      999995         keith-baker.com\n",
      "999995      999996              kireie.com\n",
      "999996      999997             mt-lock.com\n",
      "999997      999998           pinkwater.com\n",
      "999998      999999          soderhomes.com\n",
      "999999     1000000  toyotamusicfactory.com\n"
     ]
    }
   ],
   "source": [
    "sorted_data = data_traffic.sort_values(by='GlobalRank')\n",
    "\n",
    "top_ten = sorted_data[['GlobalRank', 'Domain']].head(10)\n",
    "bottom_ten = sorted_data[['GlobalRank', 'Domain']].tail(10)\n",
    "\n",
    "print(\"Top Ten Domain Names by GlobalRank:\")\n",
    "print(top_ten)\n",
    "\n",
    "print(\"\\nBottom Ten Domain Names by GlobalRank:\")\n",
    "print(bottom_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618e5e0-9891-45a0-a251-a899e4ce3f80",
   "metadata": {},
   "source": [
    "# Task 1: Countries with the highest number of news media organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925a4abc-e977-4f58-a841-1bfddd6651ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ten Countries with the highest number of news media organizations:\n",
      "Country\n",
      "United States     14111\n",
      "United Kingdom     1946\n",
      "Italy              1804\n",
      "France             1039\n",
      "Russia             1020\n",
      "Canada              886\n",
      "Germany             884\n",
      "China               779\n",
      "Turkey              725\n",
      "India               686\n",
      "Name: SourceCommonName, dtype: int64\n",
      "\n",
      "Bottom Ten Countries with the lowest number of news media organizations:\n",
      "Country\n",
      "Guinea-Bissau               1\n",
      "Guernsey                    1\n",
      "Mayotte                     1\n",
      "Oceans                      1\n",
      "Greenland                   1\n",
      "Aruba                       1\n",
      "Saint Helena                1\n",
      "Guadeloupe                  1\n",
      "Cook Islands                1\n",
      "Turks and Caicos Islands    1\n",
      "Name: SourceCommonName, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "country_counts = data_domain.groupby('Country')['SourceCommonName'].nunique()\n",
    "sorted_country_counts = country_counts.sort_values(ascending=False)\n",
    "top_countries = sorted_country_counts.head(10)\n",
    "bottom_countries = sorted_country_counts.tail(10)\n",
    "print(\"Top Ten Countries with the highest number of news media organizations:\")\n",
    "print(top_countries)\n",
    "print(\"\\nBottom Ten Countries with the lowest number of news media organizations:\")\n",
    "print(bottom_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6793b2-0a14-47c1-94d4-8657de4113f5",
   "metadata": {},
   "source": [
    "# Task 1: Countries that have many articles written about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ef1e9e7-3630-46d7-b816-d0202b4901d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ten Countries with the highest news counts:\n",
      "category\n",
      "Canada            2066\n",
      "India             1054\n",
      "Australia          877\n",
      "United Kingdom     753\n",
      "Ukraine            577\n",
      "Mexico             553\n",
      "United States      481\n",
      "Nigeria            469\n",
      "Germany            459\n",
      "Hong Kong          430\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom Ten Countries with the lowest news counts:\n",
      "category\n",
      "Liechtenstein    8\n",
      "Bhutan           7\n",
      "Montserrat       7\n",
      "Eritrea          6\n",
      "Gambia           4\n",
      "San Marino       4\n",
      "Honduras         3\n",
      "Andorra          3\n",
      "Burundi          3\n",
      "Martinique       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "country_names = data_domain['Country'].tolist()\n",
    "country_data = data[data['category'].isin(country_names)]\n",
    "country_talking_counts = country_data['category'].value_counts()\n",
    "sorted_country_talking = country_talking_counts.sort_values(ascending=False)\n",
    "top_10_countries = sorted_country_talking.head(10)\n",
    "bottom_10_countries = sorted_country_talking.tail(10)\n",
    "\n",
    "print(\"Top Ten Countries with the highest news counts:\")\n",
    "print(top_10_countries)\n",
    "\n",
    "print(\"\\nBottom Ten Countries with the lowest news counts:\")\n",
    "print(bottom_10_countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ac102-8bb9-4a8a-b5c7-d838fc4182cd",
   "metadata": {},
   "source": [
    "# Task 1: Websites that reported (the news content) about Africa, US, China, EU, Russia, Ukraine, Middle East?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a43dad-69b4-4837-ae69-69c67b3c2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "websites who are talking about African countries\n",
      "\n",
      "Top ten websites\n",
      "website\n",
      "punchng.com                     839\n",
      "www.marketscreener.com          248\n",
      "www.globalsecurity.org          202\n",
      "www.etfdailynews.com            190\n",
      "www.bbc.co.uk                   136\n",
      "www.rt.com                      130\n",
      "economictimes.indiatimes.com    122\n",
      "www.aljazeera.com               117\n",
      "abcnews.go.com                  116\n",
      "www.ibtimes.com                  98\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom ten websites\n",
      "website\n",
      "time.com                  21\n",
      "indianexpress.com         15\n",
      "boingboing.net             8\n",
      "gizmodo.com                5\n",
      "www.digitaltrends.com      3\n",
      "www.androidcentral.com     2\n",
      "www.wired.com              2\n",
      "readwrite.com              2\n",
      "www.bbc.com                2\n",
      "www.theverge.com           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "websites who are talking about Europian countries\n",
      "\n",
      "Top ten websites\n",
      "website\n",
      "www.etfdailynews.com       1072\n",
      "www.globalsecurity.org      573\n",
      "www.globenewswire.com       571\n",
      "www.rt.com                  279\n",
      "abcnews.go.com              276\n",
      "www.businessinsider.com     230\n",
      "www.aljazeera.com           222\n",
      "www.bbc.co.uk               176\n",
      "www.euronews.com            168\n",
      "www.ibtimes.com             155\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom ten websites\n",
      "website\n",
      "indianexpress.com         37\n",
      "boingboing.net            32\n",
      "www.cnn.com               27\n",
      "www.digitaltrends.com     17\n",
      "www.bbc.com               13\n",
      "www.wired.com              9\n",
      "readwrite.com              9\n",
      "gizmodo.com                9\n",
      "www.androidcentral.com     6\n",
      "www.theverge.com           4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "websites who are talking about Middle East\n",
      "\n",
      "Top ten websites\n",
      "website\n",
      "www.globalsecurity.org          350\n",
      "www.aljazeera.com               237\n",
      "economictimes.indiatimes.com    145\n",
      "www.businessinsider.com         122\n",
      "www.rt.com                      121\n",
      "abcnews.go.com                  118\n",
      "www.bbc.co.uk                    90\n",
      "www.npr.org                      85\n",
      "www.ibtimes.com                  72\n",
      "www.globenewswire.com            70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom ten websites\n",
      "website\n",
      "phys.org                  21\n",
      "www.cnn.com               15\n",
      "www.euronews.com          15\n",
      "www.digitaltrends.com     13\n",
      "www.bbc.com                4\n",
      "readwrite.com              1\n",
      "www.wired.com              1\n",
      "www.androidcentral.com     1\n",
      "gizmodo.com                1\n",
      "www.theverge.com           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "websites who are talking about United State\n",
      "\n",
      "Top ten websites\n",
      "website\n",
      "www.etfdailynews.com            145\n",
      "www.globenewswire.com            78\n",
      "www.globalsecurity.org           47\n",
      "www.businessinsider.com          35\n",
      "boingboing.net                   28\n",
      "abcnews.go.com                   22\n",
      "www.npr.org                      18\n",
      "www.aljazeera.com                11\n",
      "www.forbes.com                   11\n",
      "economictimes.indiatimes.com     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom ten websites\n",
      "website\n",
      "gizmodo.com               6\n",
      "www.androidcentral.com    5\n",
      "www.ibtimes.com           5\n",
      "www.wired.com             4\n",
      "www.theverge.com          3\n",
      "www.rt.com                3\n",
      "punchng.com               3\n",
      "deadline.com              2\n",
      "www.cnn.com               2\n",
      "indianexpress.com         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "websites who are talking about China\n",
      "\n",
      "Top ten websites\n",
      "website\n",
      "www.globalsecurity.org          113\n",
      "www.etfdailynews.com             79\n",
      "timesofindia.indiatimes.com      38\n",
      "economictimes.indiatimes.com     31\n",
      "www.ibtimes.com                  22\n",
      "www.rt.com                       21\n",
      "www.globenewswire.com            20\n",
      "www.aljazeera.com                18\n",
      "www.businessinsider.com          15\n",
      "www.forbes.com                   11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom ten websites\n",
      "website\n",
      "punchng.com               5\n",
      "deadline.com              4\n",
      "www.androidcentral.com    4\n",
      "www.bbc.co.uk             4\n",
      "indianexpress.com         3\n",
      "boingboing.net            3\n",
      "www.cnn.com               2\n",
      "gizmodo.com               1\n",
      "phys.org                  1\n",
      "www.bbc.com               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "websites who are talking about Russia\n",
      "\n",
      "Top ten websites\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Bottom ten websites\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "websites who are talking about Ukraine\n",
      "\n",
      "Top ten websites\n",
      "website\n",
      "www.globalsecurity.org          169\n",
      "www.businessinsider.com         151\n",
      "www.rt.com                       61\n",
      "abcnews.go.com                   51\n",
      "www.bbc.co.uk                    31\n",
      "timesofindia.indiatimes.com      28\n",
      "www.npr.org                      16\n",
      "www.ibtimes.com                  15\n",
      "economictimes.indiatimes.com     13\n",
      "www.aljazeera.com                10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom ten websites\n",
      "website\n",
      "readwrite.com            4\n",
      "boingboing.net           4\n",
      "www.globenewswire.com    4\n",
      "www.wired.com            3\n",
      "gizmodo.com              2\n",
      "phys.org                 2\n",
      "www.forbes.com           2\n",
      "www.bbc.com              2\n",
      "www.theverge.com         1\n",
      "punchng.com              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "african_countries = [\"Algeria\", \"Angola\", \"Benin\", \"Botswana\", \"Burkina Faso\", \"Burundi\", \"Cabo Verde\", \"Cameroon\", \"Central African Republic\", \"Chad\", \"Comoros\", \"Congo\",\n",
    "\"Democratic Republic of the Congo\", \"Djibouti\", \"Egypt\", \"Equatorial Guinea\", \"Eritrea\", \"Ethiopia\", \"Eswatini\", \"Gabon\", \"Gambia\", \"Ghana\", \"Guinea\", \n",
    "\"Guinea-Bissau\", \"Ivory Coast\", \"Kenya\", \"Lesotho\", \"Liberia\", \"Libya\", \"Madagascar\", \"Malawi\", \"Mali\", \"Mauritania\", \"Mauritius\", \"Morocco\", \n",
    "\"Mozambique\", \"Namibia\", \"Niger\", \"Nigeria\", \"Rwanda\", \"São Tomé and Príncipe\", \"Senegal\", \"Seychelles\", \"Sierra Leone\", \"Somalia\", \"South Africa\", \n",
    "\"South Sudan\", \"Sudan\", \"Tanzania\", \"Togo\", \"Tunisia\", \"Uganda\", \"Zambia\", \"Zimbabwe\"]\n",
    "\n",
    "europian_countries= [\"Albania\", \"Andorra\", \"Armenia\", \"Austria\", \"Azerbaijan\", \"Belarus\", \"Belgium\", \"Bosnia and Herzegovina\", \"Bulgaria\", \"Croatia\", \n",
    "\"Cyprus\", \"Czech Republic\", \"Denmark\", \"Estonia\", \"Finland\", \"France\", \"Georgia\", \"Germany\", \"Greece\", \"Hungary\", \"Iceland\",\n",
    "\"Ireland\", \"Italy\", \"Kosovo\", \"Latvia\", \"Liechtenstein\", \"Lithuania\", \"Luxembourg\", \"Malta\", \"Moldova\", \"Monaco\", \"Montenegro\",\n",
    "\"Netherlands\", \"North Macedonia\", \"Norway\", \"Poland\", \"Portugal\", \"Romania\", \"Russia\", \"San Marino\", \"Serbia\", \"Slovakia\", \"Slovenia\", \"Spain\",\n",
    "\"Sweden\", \"Switzerland\", \"Turkey\", \"Ukraine\", \"United Kingdom\"]\n",
    "\n",
    "middle_east =[\"Bahrain\", \"Cyprus\", \"Egypt\", \"Iran\", \"Iraq\", \"Israel\", \"Jordan\", \"Kuwait\", \"Lebanon\", \"Oman\", \"Qatar\", \"Saudi Arabia\", \"Syria\",\n",
    "\"Turkey\", \"United Arab Emirates\", \"Yemen\"]\n",
    "\n",
    "united_states = ['United States']\n",
    "china = ['China']\n",
    "russia = ['Russia']\n",
    "ukraine = ['Ukraine']\n",
    "def news_count_per_group(group_name, data):\n",
    "    group_data = data[data['category'].isin(group_name)]\n",
    "    group_website_counts = group_data['website'].value_counts()\n",
    "    sorted_group_website_count = group_website_counts.sort_values(ascending=False)\n",
    "    top_10_group_websites = sorted_group_website_count.head(10)\n",
    "    bottom_10_group_websites = sorted_group_website_count.tail(10)\n",
    "\n",
    "    print('\\nTop ten websites')\n",
    "    print(top_10_group_websites)\n",
    "\n",
    "    print('\\nBottom ten websites')\n",
    "    print(bottom_10_group_websites)\n",
    "\n",
    "# Call the function with the list of African countries and your data\n",
    "print('websites who are talking about African countries')\n",
    "news_count_per_group(african_countries, data)\n",
    "\n",
    "print('\\nwebsites who are talking about Europian countries')\n",
    "news_count_per_group(europian_countries, data)\n",
    "\n",
    "print('\\nwebsites who are talking about Middle East')\n",
    "news_count_per_group(middle_east, data)\n",
    "\n",
    "print('\\nwebsites who are talking about United State')\n",
    "news_count_per_group(united_states, data)\n",
    "\n",
    "print('\\nwebsites who are talking about China')\n",
    "news_count_per_group(china, data)\n",
    "\n",
    "print('\\nwebsites who are talking about Russia')\n",
    "news_count_per_group(russia, data)\n",
    "\n",
    "print('\\nwebsites who are talking about Ukraine')\n",
    "news_count_per_group(ukraine, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea98db-c972-4b50-a92b-cb230d2684b9",
   "metadata": {},
   "source": [
    "# Task 1: Websites with the highest count of positive, neutral, and negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0980edff-f1c5-4e22-8cfb-2bba201cccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ten Websites:\n",
      "title_sentiment               Negative  Neutral  Positive         mean\n",
      "website                                                               \n",
      "www.etfdailynews.com               560    15194       992  5582.000000\n",
      "www.globenewswire.com               27     4491       905  1807.666667\n",
      "economictimes.indiatimes.com       744     3634       932  1770.000000\n",
      "www.globalsecurity.org             747     2255       117  1039.666667\n",
      "www.forbes.com                     216     1933       635   928.000000\n",
      "timesofindia.indiatimes.com        466     1515       213   731.333333\n",
      "abcnews.go.com                     726     1193       139   686.000000\n",
      "www.businessinsider.com            890      907       237   678.000000\n",
      "www.bbc.co.uk                      828     1135        69   677.333333\n",
      "punchng.com                        455     1229       116   600.000000\n",
      "\n",
      "Bottom Ten Websites:\n",
      "title_sentiment                   Negative  Neutral  Positive        mean\n",
      "website                                                                  \n",
      "gizmodo.com                             76      253        59  129.333333\n",
      "readwrite.com                           38      216        70  108.000000\n",
      "www.euronews.com                       114      157        15   95.333333\n",
      "www.wired.com                           65      138        67   90.000000\n",
      "www.cnn.com                            102      142        23   89.000000\n",
      "www.theverge.com                        35      118        61   71.333333\n",
      "www.bbc.com                             26       50         5   27.000000\n",
      "allafrica.com                            8       12         0    6.666667\n",
      "cnalifestyle.channelnewsasia.com         0       12         6    6.000000\n",
      "cnaluxury.channelnewsasia.com            0        2         2    1.333333\n"
     ]
    }
   ],
   "source": [
    "grouped_data = data.groupby(['website', 'title_sentiment'])\n",
    "sentiment_counts = grouped_data.size().unstack(fill_value=0)\n",
    "sentiment_counts['mean'] = sentiment_counts.mean(axis=1)\n",
    "sorted_sentiment_counts = sentiment_counts.sort_values(by='mean', ascending=False)\n",
    "top_ten = sorted_sentiment_counts.head(10)\n",
    "bottom_ten = sorted_sentiment_counts.tail(10)\n",
    "print(\"Top Ten Websites:\")\n",
    "print(top_ten)\n",
    "print(\"\\nBottom Ten Websites:\")\n",
    "print(bottom_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143285d4-d7a2-44ef-bf79-4d58df2f7b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_sentiment                   Negative  Neutral  Positive\n",
      "website                                                      \n",
      "abcnews.go.com                         726     1193       139\n",
      "allafrica.com                            8       12         0\n",
      "boingboing.net                         250      293       170\n",
      "cnalifestyle.channelnewsasia.com         0       12         6\n",
      "cnaluxury.channelnewsasia.com            0        2         2\n",
      "deadline.com                            84      794        54\n",
      "economictimes.indiatimes.com           744     3634       932\n",
      "gizmodo.com                             76      253        59\n",
      "indianexpress.com                      251      787       138\n",
      "phys.org                               284      836       143\n",
      "punchng.com                            455     1229       116\n",
      "readwrite.com                           38      216        70\n",
      "time.com                               185      382        33\n",
      "timesofindia.indiatimes.com            466     1515       213\n",
      "www.aljazeera.com                      706      927        31\n",
      "www.androidcentral.com                  38      212       272\n",
      "www.bbc.co.uk                          828     1135        69\n",
      "www.bbc.com                             26       50         5\n",
      "www.businessinsider.com                890      907       237\n",
      "www.channelnewsasia.com                126      448        78\n",
      "www.cnn.com                            102      142        23\n",
      "www.digitaltrends.com                   43      335       406\n",
      "www.etfdailynews.com                   560    15194       992\n",
      "www.euronews.com                       114      157        15\n",
      "www.forbes.com                         216     1933       635\n",
      "www.globalsecurity.org                 747     2255       117\n",
      "www.globenewswire.com                   27     4491       905\n",
      "www.ibtimes.com                        319      844        44\n",
      "www.marketscreener.com                  42     1167       153\n",
      "www.npr.org                            279      617        85\n",
      "www.rt.com                             403      698        27\n",
      "www.theverge.com                        35      118        61\n",
      "www.wired.com                           65      138        67\n"
     ]
    }
   ],
   "source": [
    "website_sentiment_counts = data.groupby(['website', 'title_sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Print the sentiment counts for each website\n",
    "print(website_sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e454016a-d0ba-446d-b72d-96ad93d06bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Negative Sentiment for Top 10 Websites:\n",
      "www.euronews.com: nan\n",
      "www.forbes.com: nan\n",
      "www.globalsecurity.org: nan\n",
      "www.globenewswire.com: nan\n",
      "www.ibtimes.com: nan\n",
      "www.marketscreener.com: nan\n",
      "www.npr.org: nan\n",
      "www.rt.com: nan\n",
      "www.theverge.com: nan\n",
      "www.wired.com: nan\n",
      "\n",
      "\n",
      "Mean Negative Sentiment for Bottom 10 Websites:\n",
      "abcnews.go.com: nan\n",
      "allafrica.com: nan\n",
      "boingboing.net: nan\n",
      "cnalifestyle.channelnewsasia.com: nan\n",
      "cnaluxury.channelnewsasia.com: nan\n",
      "deadline.com: nan\n",
      "economictimes.indiatimes.com: nan\n",
      "gizmodo.com: nan\n",
      "indianexpress.com: nan\n",
      "phys.org: nan\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'title_sentiment' is negative\n",
    "negative_sentiment_data = data[data['title_sentiment'] == 'negative']\n",
    "\n",
    "# Group by 'website' and count occurrences of negative sentiment\n",
    "negative_sentiment_counts = negative_sentiment_data.groupby('website').size()\n",
    "\n",
    "# Calculate the total number of titles for each website\n",
    "total_titles_per_website = data.groupby('website').size()\n",
    "\n",
    "# Calculate the mean negative sentiment for each website\n",
    "mean_negative_sentiment = negative_sentiment_counts / total_titles_per_website\n",
    "\n",
    "# Sort mean negative sentiment values in ascending order\n",
    "mean_negative_sentiment_sorted = mean_negative_sentiment.sort_values()\n",
    "\n",
    "# Get top 10 and bottom 10 websites\n",
    "top_10_websites = mean_negative_sentiment_sorted.tail(10)\n",
    "bottom_10_websites = mean_negative_sentiment_sorted.head(10)\n",
    "\n",
    "# Print mean negative sentiment for top 10 websites\n",
    "print(\"Mean Negative Sentiment for Top 10 Websites:\")\n",
    "for website in top_10_websites.index:\n",
    "    print(f\"{website}: {top_10_websites[website]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print mean negative sentiment for bottom 10 websites\n",
    "print(\"Mean Negative Sentiment for Bottom 10 Websites:\")\n",
    "for website in bottom_10_websites.index:\n",
    "    print(f\"{website}: {bottom_10_websites[website]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b679d5d-0ef3-4a66-b515-767c11d1ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Mean Negative Sentiment:\n",
      "website\n",
      "abcnews.go.com                      0.352770\n",
      "www.rt.com                          0.357270\n",
      "www.cnn.com                         0.382022\n",
      "www.euronews.com                    0.398601\n",
      "allafrica.com                       0.400000\n",
      "www.bbc.co.uk                       0.407480\n",
      "www.aljazeera.com                   0.424279\n",
      "www.businessinsider.com             0.437561\n",
      "cnalifestyle.channelnewsasia.com         NaN\n",
      "cnaluxury.channelnewsasia.com            NaN\n",
      "dtype: float64\n",
      "\n",
      "Bottom 10 Mean Negative Sentiment:\n",
      "website\n",
      "www.globenewswire.com           0.004979\n",
      "www.marketscreener.com          0.030837\n",
      "www.etfdailynews.com            0.033441\n",
      "www.digitaltrends.com           0.054847\n",
      "www.androidcentral.com          0.072797\n",
      "www.forbes.com                  0.077586\n",
      "deadline.com                    0.090129\n",
      "readwrite.com                   0.117284\n",
      "economictimes.indiatimes.com    0.140113\n",
      "www.theverge.com                0.163551\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'title_sentiment' is negative\n",
    "negative_sentiment_data = data[data['title_sentiment'] == 'Negative']\n",
    "\n",
    "# Group by 'website' and count occurrences of negative sentiment\n",
    "negative_sentiment_counts = negative_sentiment_data.groupby('website').size()\n",
    "\n",
    "# Calculate the total number of titles for each website\n",
    "total_titles_per_website = data.groupby('website').size()\n",
    "\n",
    "# Calculate the mean negative sentiment for each website\n",
    "mean_negative_sentiment = negative_sentiment_counts / total_titles_per_website\n",
    "\n",
    "# Sort mean negative sentiment values in ascending order\n",
    "mean_negative_sentiment_sorted = mean_negative_sentiment.sort_values()\n",
    "\n",
    "# Print the top 10 and bottom 10 mean negative sentiment values\n",
    "print(\"Top 10 Mean Negative Sentiment:\")\n",
    "print(mean_negative_sentiment_sorted.tail(10))\n",
    "print(\"\\nBottom 10 Mean Negative Sentiment:\")\n",
    "print(mean_negative_sentiment_sorted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c5897d7-c480-4fda-9201-c6bfaf5ad2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 websites with the lowest mean negative sentiment:\n",
      "website\n",
      "www.globenewswire.com           0.004979\n",
      "www.marketscreener.com          0.030837\n",
      "www.etfdailynews.com            0.033441\n",
      "www.digitaltrends.com           0.054847\n",
      "www.androidcentral.com          0.072797\n",
      "www.forbes.com                  0.077586\n",
      "deadline.com                    0.090129\n",
      "readwrite.com                   0.117284\n",
      "economictimes.indiatimes.com    0.140113\n",
      "www.theverge.com                0.163551\n",
      "dtype: float64\n",
      "\n",
      "Top 10 websites with the highest mean negative sentiment:\n",
      "website\n",
      "www.businessinsider.com    0.437561\n",
      "www.aljazeera.com          0.424279\n",
      "www.bbc.co.uk              0.407480\n",
      "allafrica.com              0.400000\n",
      "www.euronews.com           0.398601\n",
      "www.cnn.com                0.382022\n",
      "www.rt.com                 0.357270\n",
      "abcnews.go.com             0.352770\n",
      "boingboing.net             0.350631\n",
      "www.bbc.com                0.320988\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'title_sentiment' is negative\n",
    "negative_sentiment_data = data[data['title_sentiment'] == 'Negative']\n",
    "\n",
    "# Group by 'website' and count occurrences of negative sentiment\n",
    "negative_sentiment_counts = negative_sentiment_data.groupby('website').size()\n",
    "\n",
    "# Calculate the total number of titles for each website\n",
    "total_titles_per_website = data.groupby('website').size()\n",
    "\n",
    "# Calculate the mean negative sentiment for each website\n",
    "mean_negative_sentiment = negative_sentiment_counts / total_titles_per_website\n",
    "\n",
    "# Sort mean negative sentiment in ascending order to find lowest\n",
    "lowest_mean_negative_sentiment = mean_negative_sentiment.sort_values().head(10)\n",
    "\n",
    "# Sort mean negative sentiment in descending order to find highest\n",
    "highest_mean_negative_sentiment = mean_negative_sentiment.sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 websites with the lowest mean negative sentiment:\")\n",
    "print(lowest_mean_negative_sentiment)\n",
    "\n",
    "print(\"\\nTop 10 websites with the highest mean negative sentiment:\")\n",
    "print(highest_mean_negative_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c249c322-f5f5-4142-8b92-fdde6f37f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Websites with Highest Mean Negative Sentiment:\n",
      "website\n",
      "www.businessinsider.com    0.437561\n",
      "www.aljazeera.com          0.424279\n",
      "www.bbc.co.uk              0.407480\n",
      "allafrica.com              0.400000\n",
      "www.euronews.com           0.398601\n",
      "www.cnn.com                0.382022\n",
      "www.rt.com                 0.357270\n",
      "abcnews.go.com             0.352770\n",
      "boingboing.net             0.350631\n",
      "www.bbc.com                0.320988\n",
      "dtype: float64\n",
      "\n",
      "Bottom 10 Websites with Lowest Mean Negative Sentiment:\n",
      "website\n",
      "readwrite.com                       0.117284\n",
      "deadline.com                        0.090129\n",
      "www.forbes.com                      0.077586\n",
      "www.androidcentral.com              0.072797\n",
      "www.digitaltrends.com               0.054847\n",
      "www.etfdailynews.com                0.033441\n",
      "www.marketscreener.com              0.030837\n",
      "www.globenewswire.com               0.004979\n",
      "cnalifestyle.channelnewsasia.com         NaN\n",
      "cnaluxury.channelnewsasia.com            NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "negative_sentiment_data = data[data['title_sentiment'] == 'Negative']\n",
    "negative_sentiment_counts = negative_sentiment_data.groupby('website').size()\n",
    "total_titles_per_website = data.groupby('website').size()\n",
    "mean_negative_sentiment = negative_sentiment_counts / total_titles_per_website\n",
    "mean_negative_sentiment_sorted = mean_negative_sentiment.sort_values(ascending=False)\n",
    "top_10_high_negative_sentiment = mean_negative_sentiment_sorted.head(10)\n",
    "bottom_10_low_negative_sentiment = mean_negative_sentiment_sorted.tail(10)\n",
    "\n",
    "print(\"Top 10 Websites with Highest Mean Negative Sentiment:\")\n",
    "print(top_10_high_negative_sentiment)\n",
    "print(\"\\nBottom 10 Websites with Lowest Mean Negative Sentiment:\")\n",
    "print(bottom_10_low_negative_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83371053-c99d-4df0-ab5f-67a0d37c929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Websites with Highest Mean Positive Sentiment:\n",
      "website\n",
      "www.androidcentral.com              0.521073\n",
      "www.digitaltrends.com               0.517857\n",
      "cnaluxury.channelnewsasia.com       0.500000\n",
      "cnalifestyle.channelnewsasia.com    0.333333\n",
      "www.theverge.com                    0.285047\n",
      "www.wired.com                       0.248148\n",
      "boingboing.net                      0.238429\n",
      "www.forbes.com                      0.228089\n",
      "readwrite.com                       0.216049\n",
      "economictimes.indiatimes.com        0.175518\n",
      "dtype: float64\n",
      "\n",
      "Bottom 10 Websites with Lowest Mean Positive Sentiment:\n",
      "website\n",
      "www.etfdailynews.com      0.059238\n",
      "deadline.com              0.057940\n",
      "time.com                  0.055000\n",
      "www.euronews.com          0.052448\n",
      "www.globalsecurity.org    0.037512\n",
      "www.ibtimes.com           0.036454\n",
      "www.bbc.co.uk             0.033957\n",
      "www.rt.com                0.023936\n",
      "www.aljazeera.com         0.018630\n",
      "allafrica.com                  NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "positive_sentiment_data = data[data['title_sentiment'] == 'Positive']\n",
    "positive_sentiment_counts = positive_sentiment_data.groupby('website').size()\n",
    "total_titles_per_website = data.groupby('website').size()\n",
    "mean_positive_sentiment = positive_sentiment_counts / total_titles_per_website\n",
    "mean_positive_sentiment_sorted = mean_positive_sentiment.sort_values(ascending=False)\n",
    "top_10_high_positive_sentiment = mean_positive_sentiment_sorted.head(10)\n",
    "bottom_10_low_positive_sentiment = mean_positive_sentiment_sorted.tail(10)\n",
    "\n",
    "print(\"Top 10 Websites with Highest Mean Positive Sentiment:\")\n",
    "print(top_10_high_positive_sentiment)\n",
    "print(\"\\nBottom 10 Websites with Lowest Mean Positive Sentiment:\")\n",
    "print(bottom_10_low_positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eaaace38-65bd-4cc5-a267-444756db0800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Websites with Highest Mean Neutral Sentiment:\n",
      "website\n",
      "www.etfdailynews.com            0.907321\n",
      "www.marketscreener.com          0.856828\n",
      "deadline.com                    0.851931\n",
      "www.globenewswire.com           0.828139\n",
      "www.globalsecurity.org          0.722988\n",
      "www.ibtimes.com                 0.699254\n",
      "www.forbes.com                  0.694325\n",
      "timesofindia.indiatimes.com     0.690520\n",
      "www.channelnewsasia.com         0.687117\n",
      "economictimes.indiatimes.com    0.684369\n",
      "dtype: float64\n",
      "\n",
      "Bottom 10 Websites with Lowest Mean Neutral Sentiment:\n",
      "website\n",
      "www.aljazeera.com                0.557091\n",
      "www.theverge.com                 0.551402\n",
      "www.euronews.com                 0.548951\n",
      "www.cnn.com                      0.531835\n",
      "www.wired.com                    0.511111\n",
      "cnaluxury.channelnewsasia.com    0.500000\n",
      "www.businessinsider.com          0.445919\n",
      "www.digitaltrends.com            0.427296\n",
      "boingboing.net                   0.410940\n",
      "www.androidcentral.com           0.406130\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "neutral_sentiment_data = data[data['title_sentiment'] == 'Neutral']\n",
    "neutral_sentiment_counts = neutral_sentiment_data.groupby('website').size()\n",
    "total_titles_per_website = data.groupby('website').size()\n",
    "mean_neutral_sentiment = neutral_sentiment_counts / total_titles_per_website\n",
    "mean_neutral_sentiment_sorted = mean_neutral_sentiment.sort_values(ascending=False)\n",
    "top_10_high_neutral_sentiment = mean_neutral_sentiment_sorted.head(10)\n",
    "bottom_10_low_neutral_sentiment = mean_neutral_sentiment_sorted.tail(10)\n",
    "\n",
    "print(\"Top 10 Websites with Highest Mean Neutral Sentiment:\")\n",
    "print(top_10_high_neutral_sentiment)\n",
    "print(\"\\nBottom 10 Websites with Lowest Mean Neutral Sentiment:\")\n",
    "print(bottom_10_low_neutral_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a1b71-b4f6-436d-a75a-f15aacd301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Frequency Analysis\n",
    "news_reporting_frequency = data.groupby('website').size()\n",
    "\n",
    "# Step 4: Impact Analysis\n",
    "# Assuming 'global_ranking' is a column representing the website's global ranking\n",
    "impact_data = pd.DataFrame({\n",
    "    'News Reporting Frequency': news_reporting_frequency,\n",
    "    'Mean Sentiment': mean_negative_sentiment,  # Assuming you have calculated mean sentiment\n",
    "    'Global Ranking': data.groupby('website')['global_ranking'].first()  # Assuming 'global_ranking' is a column\n",
    "})\n",
    "\n",
    "# Step 5: Visualization\n",
    "# You can visualize the impact using plots or graphs, for example:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(impact_data['News Reporting Frequency'], impact_data['Global Ranking'], c=impact_data['Mean Sentiment'], cmap='coolwarm')\n",
    "plt.xlabel('News Reporting Frequency')\n",
    "plt.ylabel('Global Ranking')\n",
    "plt.title('Impact of News Reporting Frequency and Sentiment on Global Ranking')\n",
    "plt.colorbar(label='Mean Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea1100ba-969f-446b-9627-011c38fbe98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'source_id', 'source_name', 'author', 'title',\n",
      "       'description', 'url', 'url_to_image', 'published_at', 'content',\n",
      "       'category', 'article', 'title_sentiment', 'website_x', 'domain',\n",
      "       'Domain', 'sentiment_score', 'GlobalRank', 'TldRank', 'TLD',\n",
      "       'RefSubNets', 'RefIPs', 'IDN_Domain', 'IDN_TLD', 'PrevGlobalRank',\n",
      "       'PrevTldRank', 'PrevRefSubNets', 'PrevRefIPs', 'website_y'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `sentiment_score` for parameter `hue`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create the scatter plot with color representing sentiment (use 'sentiment_score' if available)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m merged_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 28\u001b[0m     sns\u001b[38;5;241m.\u001b[39mscatterplot(\n\u001b[0;32m     29\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobalRank\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m         y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDomain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m         palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Adjust palette as desired\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Adjust size based on sentiment score if needed\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata_to_plot,\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport_count\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for color as sentiment_score unavailable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\relational.py:742\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[0;32m    733\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    734\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    739\u001b[0m ):\n\u001b[0;32m    741\u001b[0m     variables \u001b[38;5;241m=\u001b[39m _ScatterPlotter\u001b[38;5;241m.\u001b[39mget_semantics(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 742\u001b[0m     p \u001b[38;5;241m=\u001b[39m _ScatterPlotter(data\u001b[38;5;241m=\u001b[39mdata, variables\u001b[38;5;241m=\u001b[39mvariables, legend\u001b[38;5;241m=\u001b[39mlegend)\n\u001b[0;32m    744\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    745\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\relational.py:538\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    535\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    536\u001b[0m     )\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data\u001b[38;5;241m=\u001b[39mdata, variables\u001b[38;5;241m=\u001b[39mvariables)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_variables(data, variables)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semantic_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     map_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmap, plotter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 701\u001b[0m     plot_data, variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    702\u001b[0m         data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvariables,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data \u001b[38;5;241m=\u001b[39m plot_data\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m variables\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret value `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for parameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, pd\u001b[38;5;241m.\u001b[39mSeries):\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `sentiment_score` for parameter `hue`"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract domain from URL (assuming URL structure is consistent)\n",
    "data[\"Domain\"] = data[\"url\"].str.extract(\"^(?:https?://)?([^/]+)\", expand=False)\n",
    "\n",
    "# Merge dataframes based on domain (assuming domains mostly match)\n",
    "merged_data = data.merge(data_traffic, on=\"Domain\", how=\"left\")\n",
    "\n",
    "# Print column names for debugging\n",
    "print(merged_data.columns)  # Check for presence of 'sentiment_score'\n",
    "\n",
    "# Handle sentiment conversion errors (assuming sentiment_map is defined)\n",
    "def convert_sentiment(text):\n",
    "    try:\n",
    "        return sentiment_map[text]\n",
    "    except KeyError:\n",
    "        print(f\"Warning: Unexpected sentiment value '{text}' encountered. Assuming neutral (score=0).\")\n",
    "        return 0  # Adjust default score if needed\n",
    "\n",
    "merged_data[\"sentiment_score\"] = merged_data[\"title_sentiment\"].apply(convert_sentiment)\n",
    "\n",
    "# Calculate count of reports per domain\n",
    "report_count = merged_data.groupby(\"Domain\")[\"title_sentiment\"].count()\n",
    "\n",
    "# Prepare data for scatter plot\n",
    "data_to_plot = pd.merge(merged_data[[\"Domain\", \"GlobalRank\"]], report_count.to_frame(\"report_count\"), on=\"Domain\")\n",
    "\n",
    "# Create the scatter plot with color representing sentiment (use 'sentiment_score' if available)\n",
    "if 'sentiment_score' in merged_data.columns:\n",
    "    sns.scatterplot(\n",
    "        x=\"GlobalRank\",\n",
    "        y=\"Domain\",\n",
    "        hue=\"sentiment_score\",\n",
    "        palette=\"coolwarm\",  # Adjust palette as desired\n",
    "        size=\"sentiment_score\",  # Adjust size based on sentiment score if needed\n",
    "        data=data_to_plot,\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Using 'report_count' for color as sentiment_score unavailable.\")\n",
    "    sns.scatterplot(\n",
    "        x=\"GlobalRank\",\n",
    "        y=\"Domain\",\n",
    "        hue=\"report_count\",  # Use report_count for color if sentiment_score unavailable\n",
    "        palette=\"coolwarm\",  # Adjust palette as desired\n",
    "        size=\"report_score\",  # Adjust size based on sentiment score if needed (assuming a score exists)\n",
    "        data=data_to_plot,\n",
    "    )\n",
    "\n",
    "# Customize plot labels and title\n",
    "plt.xlabel(\"Global Rank\")\n",
    "plt.ylabel(\"Website Domain\")\n",
    "plt.title(\"Global Rank vs. Report Count with Sentiment\")\n",
    "# Adjust legend title based on used variable (sentiment_score or report_count)\n",
    "plt.legend(title=\"Sentiment Score\" if 'sentiment_score' in merged_data.columns else \"Report Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cf290f48-9ba3-43d5-93ec-e6da595460d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2901c581-bb9e-430c-a72a-98bc586d938f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        superstar chef yannick alléno brings refined f...\n",
       "1        nice claim top spot in ligue 1 with late win a...\n",
       "2        amphibians are the world’s most vulnerable spe...\n",
       "3                    image: rusty red waters in madagascar\n",
       "4        everything leaving max (formerly hbo max) in n...\n",
       "                               ...                        \n",
       "58351    Have done no wrong, only did party work, says ...\n",
       "58352    FC Barcelona Guarantees $77.6 Million Champion...\n",
       "58353    Three hospitals ignored her gravely ill fiancé...\n",
       "58354    Kerber’s Farm: Bringing Farm To Table To Manha...\n",
       "58355    Tips For Investing In Short-Term Rentals In Dubai\n",
       "Name: title, Length: 58356, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd35e572-b16d-496e-80c8-12f44201c104",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m titles \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m wordDictA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(titles, \u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m titles:\n\u001b[0;32m      4\u001b[0m     wordDictA[word]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "titles = data[\"title\"].apply(lambda x: x.split(\" \"))\n",
    "wordDictA = dict.fromkeys(titles, 0) \n",
    "for word in titles:\n",
    "    wordDictA[word]+=1\n",
    "pd.DataFrame([wordDictA])\n",
    "for word in description:\n",
    "    wordDictA[word]+=1\n",
    "pd.DataFrame([wordDictA])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1edfdf23-7dde-4c7e-b6e1-a4f24545a29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_to_image</th>\n",
       "      <th>published_at</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>article</th>\n",
       "      <th>title_sentiment</th>\n",
       "      <th>website</th>\n",
       "      <th>domain</th>\n",
       "      <th>Domain</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>article_clean</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Elizabeth Brownfield, Contributor, \\n Elizabet...</td>\n",
       "      <td>superstar chef yannick alléno brings refined f...</td>\n",
       "      <td>Now open in Mayfair at Four Seasons Hotel Lond...</td>\n",
       "      <td>https://www.forbes.com/sites/elizabethbrownfie...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2023-11-01 03:27:21.000000</td>\n",
       "      <td>Pavyllon London, at Four Seasons Hotel London ...</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>pavyllon london, at four seasons hotel london ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>www.forbes.com</td>\n",
       "      <td>www.forbes.com</td>\n",
       "      <td>www.forbes.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>superstar chef yannick alléno brings refined f...</td>\n",
       "      <td>pavyllon london, at four seasons hotel london ...</td>\n",
       "      <td>now open in mayfair at four seasons hotel lond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nice claim top spot in ligue 1 with late win a...</td>\n",
       "      <td>Nice moved into provisional first place in the...</td>\n",
       "      <td>https://www.channelnewsasia.com/sport/nice-cla...</td>\n",
       "      <td>https://onecms-res.cloudinary.com/image/upload...</td>\n",
       "      <td>2023-10-27 21:28:48.000000</td>\n",
       "      <td>Nice moved into provisional first place in the...</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>nice moved into provisional first place in the...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>www.channelnewsasia.com</td>\n",
       "      <td>www.channelnewsasia.com</td>\n",
       "      <td>www.channelnewsasia.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nice claim top spot in ligue 1 with late win a...</td>\n",
       "      <td>nice moved into provisional first place in the...</td>\n",
       "      <td>nice moved into provisional first place in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id source_id source_name  \\\n",
       "0       81664       NaN      Forbes   \n",
       "1       81667       NaN         CNA   \n",
       "\n",
       "                                              author  \\\n",
       "0  Elizabeth Brownfield, Contributor, \\n Elizabet...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0  superstar chef yannick alléno brings refined f...   \n",
       "1  nice claim top spot in ligue 1 with late win a...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Now open in Mayfair at Four Seasons Hotel Lond...   \n",
       "1  Nice moved into provisional first place in the...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.forbes.com/sites/elizabethbrownfie...   \n",
       "1  https://www.channelnewsasia.com/sport/nice-cla...   \n",
       "\n",
       "                                        url_to_image  \\\n",
       "0  https://imageio.forbes.com/specials-images/ima...   \n",
       "1  https://onecms-res.cloudinary.com/image/upload...   \n",
       "\n",
       "                 published_at  \\\n",
       "0  2023-11-01 03:27:21.000000   \n",
       "1  2023-10-27 21:28:48.000000   \n",
       "\n",
       "                                             content category  \\\n",
       "0  Pavyllon London, at Four Seasons Hotel London ...   Monaco   \n",
       "1  Nice moved into provisional first place in the...   Monaco   \n",
       "\n",
       "                                             article title_sentiment  \\\n",
       "0  pavyllon london, at four seasons hotel london ...         Neutral   \n",
       "1  nice moved into provisional first place in the...        Positive   \n",
       "\n",
       "                   website                   domain                   Domain  \\\n",
       "0           www.forbes.com           www.forbes.com           www.forbes.com   \n",
       "1  www.channelnewsasia.com  www.channelnewsasia.com  www.channelnewsasia.com   \n",
       "\n",
       "   sentiment_score                                        title_clean  \\\n",
       "0              NaN  superstar chef yannick alléno brings refined f...   \n",
       "1              NaN  nice claim top spot in ligue 1 with late win a...   \n",
       "\n",
       "                                       article_clean  \\\n",
       "0  pavyllon london, at four seasons hotel london ...   \n",
       "1  nice moved into provisional first place in the...   \n",
       "\n",
       "                                   description_clean  \n",
       "0  now open in mayfair at four seasons hotel lond...  \n",
       "1  nice moved into provisional first place in the...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ea1b971f-08a5-4f9a-95a3-a18ac58430df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   superstar  chef  yannick  alléno  brings  refined  french  cuisine     to  \\\n",
      "0          8     9        1       1      45        2      42        2  15221   \n",
      "\n",
      "   pavyllon  ...  Rausch  Marcia  freely:  opposing  Guarantees  $77.6  \\\n",
      "0         1  ...       1       1        1         1           1      1   \n",
      "\n",
      "   Packet  fiancé.  Kerber’s  Manhattan’s  \n",
      "0       1        1         1            1  \n",
      "\n",
      "[1 rows x 89952 columns]\n"
     ]
    }
   ],
   "source": [
    "wordDictA = {}\n",
    "\n",
    "# Iterate over each title (list of words) in split_notes\n",
    "for title_words in split_notes:\n",
    "  # Iterate over each word in the title\n",
    "  for word in title_words:\n",
    "    # Update the word count in wordDictA\n",
    "    wordDictA[word] = wordDictA.get(word, 0) + 1\n",
    "\n",
    "# Convert the dictionary to a DataFrame (optional)\n",
    "df = pd.DataFrame([wordDictA])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8325f67f-1282-4c44-85ae-32bd59bf6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, titles)\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0fdeb335-acfc-4b75-99e7-03e44d99eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "set(stopwords.words('english'))\n",
    "topwords_english = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1660802-7175-4805-990b-9240af98bddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e86d39-dd63-4664-94a6-89adab32548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ten website by number of article published:\n",
      "website\n",
      "www.etfdailynews.com            16746\n",
      "www.globenewswire.com            5423\n",
      "economictimes.indiatimes.com     5310\n",
      "www.globalsecurity.org           3119\n",
      "www.forbes.com                   2784\n",
      "timesofindia.indiatimes.com      2194\n",
      "abcnews.go.com                   2058\n",
      "www.businessinsider.com          2034\n",
      "www.bbc.co.uk                    2032\n",
      "punchng.com                      1800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom Ten websites by number of article published:\n",
      "website\n",
      "gizmodo.com                         388\n",
      "readwrite.com                       324\n",
      "www.euronews.com                    286\n",
      "www.wired.com                       270\n",
      "www.cnn.com                         267\n",
      "www.theverge.com                    214\n",
      "www.bbc.com                          81\n",
      "allafrica.com                        20\n",
      "cnalifestyle.channelnewsasia.com     18\n",
      "cnaluxury.channelnewsasia.com         4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "def extract_website(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "data['website'] = data['url'].apply(extract_website)\n",
    "website_counts = data['website'].value_counts()\n",
    "sorted_website_data = website_counts.sort_values(ascending=False)\n",
    "top_websites = sorted_website_data.head(10)\n",
    "bottom_websites = sorted_website_data.tail(10)\n",
    "print(\"Top Ten website by number of article published:\")\n",
    "print(top_websites)\n",
    "print(\"\\nBottom Ten websites by number of article published:\")\n",
    "print(bottom_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2bd3226d-7952-4aaa-bfdf-49324a51427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Terms:\n",
      "  - to: 16841\n",
      "  - in: 16001\n",
      "  - of: 13567\n",
      "  - the: 10627\n",
      "  - inc: 7469\n",
      "  - and: 6966\n",
      "  - for: 6962\n",
      "  - by: 6400\n",
      "  - a: 5854\n",
      "  - on: 5342\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Define a function to clean text (optional)\n",
    "def clean_text(text):\n",
    "  import string\n",
    "  text = text.lower()\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  return text.split()\n",
    "\n",
    "def calculate_term_frequencies(titles):\n",
    "  \"\"\"\n",
    "  This function calculates term frequencies (TF) across websites\n",
    "  in a pandas Series of titles.\n",
    "\n",
    "  Args:\n",
    "      titles: A pandas Series containing website titles.\n",
    "\n",
    "  Returns:\n",
    "      A Counter object containing word frequencies.\n",
    "  \"\"\"\n",
    "\n",
    "  # Combine all titles (efficient for large datasets)\n",
    "  all_titles_text = \" \".join(titles)\n",
    "\n",
    "  # Clean text (optional)\n",
    "  words = clean_text(all_titles_text)\n",
    "\n",
    "  # Count word occurrences using Counter\n",
    "  word_counts = Counter(words)\n",
    "\n",
    "  return word_counts  # Return the Counter object\n",
    "titles = data[\"title\"]\n",
    "term_frequencies = calculate_term_frequencies(titles)\n",
    "top_10_terms = term_frequencies.most_common(10)  # Change 10 to your desired number\n",
    "print(\"Top 10 Most Frequent Terms:\")\n",
    "for word, count in top_10_terms:\n",
    "  print(f\"  - {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0787178-88b7-4add-b5ac-88d61f174bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequent terms across websites:\n",
      "to: 15231\n",
      "in: 14938\n",
      "of: 12822\n",
      "the: 8011\n",
      "Inc.: 6559\n",
      "and: 6318\n",
      "for: 6219\n",
      "by: 5821\n",
      "on: 4899\n",
      "a: 4863\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store term frequency for each website\n",
    "website_term_freq = {}\n",
    "\n",
    "# Splitting the title column into individual words and calculating term frequency for each website\n",
    "for website, group in data.groupby('website'):\n",
    "    all_words = []\n",
    "    for title in group['title']:\n",
    "        all_words.extend(title.split())\n",
    "    term_freq = Counter(all_words)\n",
    "    website_term_freq[website] = term_freq\n",
    "\n",
    "# Aggregate term frequencies across all websites\n",
    "all_terms_freq = Counter()\n",
    "for term_freq in website_term_freq.values():\n",
    "    all_terms_freq += term_freq\n",
    "\n",
    "# Print out the top 10 frequent terms across websites\n",
    "print(\"Top 10 frequent terms across websites:\")\n",
    "for term, frequency in all_terms_freq.most_common(10):\n",
    "    print(f\"{term}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713155c2-e277-41f3-a6fb-8677edbbcb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequent terms across websites (excluding stopwords):\n",
      "Inc.: 6559\n",
      "Shares: 4202\n",
      "Stock: 3772\n",
      "LLC: 2923\n",
      "&: 2593\n",
      "Market: 2363\n",
      "Co.: 2195\n",
      "Holdings: 2127\n",
      "Management: 2018\n",
      "New: 1967\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize a dictionary to store term frequency for each website\n",
    "website_term_freq = {}\n",
    "\n",
    "# Splitting the title column into individual words, removing stopwords, and calculating term frequency for each website\n",
    "for website, group in data.groupby('website'):\n",
    "    all_words = []\n",
    "    for title in group['title']:\n",
    "        # Split title into words and remove stopwords\n",
    "        words = [word for word in title.split() if word.lower() not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    term_freq = Counter(all_words)\n",
    "    website_term_freq[website] = term_freq\n",
    "\n",
    "# Aggregate term frequencies across all websites\n",
    "all_terms_freq = Counter()\n",
    "for term_freq in website_term_freq.values():\n",
    "    all_terms_freq += term_freq\n",
    "\n",
    "# Print out the top 10 frequent terms across websites\n",
    "print(\"Top 10 frequent terms across websites (excluding stopwords):\")\n",
    "for term, frequency in all_terms_freq.most_common(10):\n",
    "    print(f\"{term}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48176d3-b003-4271-b564-4dd789ee19e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
